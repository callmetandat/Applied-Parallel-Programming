{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from model_numba.Layers.Layers import batchNorm2D, Convolution2D_GPU\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from model_numba.unet_cuda import Double_Conv, Unet_Cuda\n",
    "from PIL import Image\n",
    "import torch\n",
    "import time\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 512, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "image = Image.open('test.jpg')\n",
    "print(image.size)\n",
    "image = transform(image).unsqueeze(0).repeat(4, 1, 1, 1)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.unet import U_net\n",
    "model = U_net(1).eval()\n",
    "weight = torch.load('weights/weights.pth')\n",
    "model.load_state_dict(weight)\n",
    "\n",
    "numba_model = Unet_Cuda(1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Runtime: 3.5811500549316406 s\n"
     ]
    },
    {
     "ename": "CudaAPIError",
     "evalue": "[2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:851\u001b[0m, in \u001b[0;36mHostOnlyCUDAMemoryManager._attempt_allocation\u001b[1;34m(self, allocator)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mallocator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CudaAPIError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# is out-of-memory?\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:1062\u001b[0m, in \u001b[0;36mNumbaCUDAMemoryManager.memalloc.<locals>.allocator\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallocator\u001b[39m():\n\u001b[1;32m-> 1062\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuMemAlloc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:327\u001b[0m, in \u001b[0;36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    326\u001b[0m retcode \u001b[38;5;241m=\u001b[39m libfn(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_ctypes_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretcode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:395\u001b[0m, in \u001b[0;36mDriver._check_ctypes_error\u001b[1;34m(self, fname, retcode)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_fork()\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m CudaAPIError(retcode, msg)\n",
      "\u001b[1;31mCudaAPIError\u001b[0m: [2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTorch Runtime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m out_numba \u001b[38;5;241m=\u001b[39m \u001b[43mnumba_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumba Runtime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Source_code\\U-Net-Background_matting\\model_numba\\unet_cuda.py:173\u001b[0m, in \u001b[0;36mUnet_Cuda.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md3, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md2, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_4\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu4)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Source_code\\U-Net-Background_matting\\model_numba\\unet_cuda.py:130\u001b[0m, in \u001b[0;36mUP.forward\u001b[1;34m(self, img, skip_img, useTorchTranspose)\u001b[0m\n\u001b[0;32m    127\u001b[0m     out_transpose \u001b[38;5;241m=\u001b[39m TransposeConvol2D_GPU(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    129\u001b[0m out_transpose \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((out_transpose, skip_img), axis\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m outConv_1 \u001b[38;5;241m=\u001b[39m \u001b[43mCombie_TileConv_GPU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_transpose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1_norm_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1_norm_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1_norm_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1_norm_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m outConv_2 \u001b[38;5;241m=\u001b[39m Combie_TileConv_GPU(outConv_1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_norm_weight,\\\n\u001b[0;32m    134\u001b[0m                                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_norm_bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_norm_mean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_norm_var, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m out_transpose, outConv_1\n",
      "File \u001b[1;32md:\\Source_code\\U-Net-Background_matting\\model_numba\\Layers\\TileConv_BatchNorm_RELU.py:69\u001b[0m, in \u001b[0;36mCombie_TileConv_GPU\u001b[1;34m(img, weight, bias, norm_weight, norm_bias, mean, var, convert_output_to_host)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Allocate and transfer data\u001b[39;00m\n\u001b[0;32m     68\u001b[0m d_img \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mto_device(img) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m img\n\u001b[1;32m---> 69\u001b[0m d_out_img \u001b[38;5;241m=\u001b[39m \u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT_CHANNEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_HEIGHT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_WIDTH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m d_weight \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mto_device(weight)\n\u001b[0;32m     71\u001b[0m d_bias \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mto_device(bias)\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:232\u001b[0m, in \u001b[0;36mrequire_context.<locals>._require_cuda_context\u001b[1;34m(*args, **kws)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_require_cuda_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkws):\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _runtime\u001b[38;5;241m.\u001b[39mensure_context():\n\u001b[1;32m--> 232\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkws\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\api.py:144\u001b[0m, in \u001b[0;36mdevice_array\u001b[1;34m(shape, dtype, strides, order, stream)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"device_array(shape, dtype=np.float64, strides=None, order='C', stream=0)\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03mAllocate an empty device ndarray. Similar to :meth:`numpy.empty`.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m shape, strides, dtype \u001b[38;5;241m=\u001b[39m prepare_shape_strides_dtype(shape, strides, dtype,\n\u001b[0;32m    143\u001b[0m                                                     order)\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdevicearray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeviceNDArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:104\u001b[0m, in \u001b[0;36mDeviceNDArrayBase.__init__\u001b[1;34m(self, shape, strides, dtype, stream, gpu_data)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpu_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malloc_size \u001b[38;5;241m=\u001b[39m _driver\u001b[38;5;241m.\u001b[39mmemory_size_from_info(\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[1;32m--> 104\u001b[0m     gpu_data \u001b[38;5;241m=\u001b[39m \u001b[43mdevices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemalloc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malloc_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malloc_size \u001b[38;5;241m=\u001b[39m _driver\u001b[38;5;241m.\u001b[39mdevice_memory_size(gpu_data)\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:1372\u001b[0m, in \u001b[0;36mContext.memalloc\u001b[1;34m(self, bytesize)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmemalloc\u001b[39m(\u001b[38;5;28mself\u001b[39m, bytesize):\n\u001b[1;32m-> 1372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemalloc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytesize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:1064\u001b[0m, in \u001b[0;36mNumbaCUDAMemoryManager.memalloc\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallocator\u001b[39m():\n\u001b[0;32m   1062\u001b[0m         driver\u001b[38;5;241m.\u001b[39mcuMemAlloc(byref(ptr), size)\n\u001b[1;32m-> 1064\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attempt_allocation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallocator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1065\u001b[0m     alloc_key \u001b[38;5;241m=\u001b[39m ptr\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m   1067\u001b[0m finalizer \u001b[38;5;241m=\u001b[39m _alloc_finalizer(\u001b[38;5;28mself\u001b[39m, ptr, alloc_key, size)\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:863\u001b[0m, in \u001b[0;36mHostOnlyCUDAMemoryManager._attempt_allocation\u001b[1;34m(self, allocator)\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeallocations\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;66;03m# try again\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mallocator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:1062\u001b[0m, in \u001b[0;36mNumbaCUDAMemoryManager.memalloc.<locals>.allocator\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallocator\u001b[39m():\n\u001b[1;32m-> 1062\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuMemAlloc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:327\u001b[0m, in \u001b[0;36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    325\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall driver api: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, libfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    326\u001b[0m retcode \u001b[38;5;241m=\u001b[39m libfn(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_ctypes_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretcode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\python3.11.0\\Lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:395\u001b[0m, in \u001b[0;36mDriver._check_ctypes_error\u001b[1;34m(self, fname, retcode)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retcode \u001b[38;5;241m==\u001b[39m enums\u001b[38;5;241m.\u001b[39mCUDA_ERROR_NOT_INITIALIZED:\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_fork()\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m CudaAPIError(retcode, msg)\n",
      "\u001b[1;31mCudaAPIError\u001b[0m: [2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "out = model(image)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Torch Runtime: {end- start} s')\n",
    "\n",
    "start = time.time()\n",
    "out_numba = numba_model(image.cpu().detach().numpy())\n",
    "end = time.time()\n",
    "\n",
    "print(f'Numba Runtime: {end- start} s')\n",
    "\n",
    "mean = np.mean(np.abs(model.d5.cpu().detach().numpy()- numba_model.d5.copy_to_host()))\n",
    "max = np.max(np.abs(model.d5.cpu().detach().numpy()- numba_model.d5.copy_to_host()))\n",
    "print(f'Mean Difference: {mean : .6f}')\n",
    "print(f'Max Difference: {max: .6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Difference:  0.000014\n",
      "Max Difference:  0.000922\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(np.abs(model.u1.cpu().detach().numpy() - numba_model.u1.copy_to_host()))\n",
    "max = np.max(np.abs(model.u1.cpu().detach().numpy() - numba_model.u1.copy_to_host()))\n",
    "print(f'Mean Difference: {mean : .6f}')\n",
    "print(f'Max Difference: {max: .6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Difference:  0.000016\n",
      "Max Difference:  0.000747\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(np.abs(model.u2.cpu().detach().numpy() - numba_model.u2.copy_to_host()))\n",
    "max = np.max(np.abs(model.u2.cpu().detach().numpy() - numba_model.u2.copy_to_host()))\n",
    "print(f'Mean Difference: {mean : .6f}')\n",
    "print(f'Max Difference: {max: .6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Difference:  0.000016\n",
      "Max Difference:  0.000887\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(np.abs(model.u3.cpu().detach().numpy() - numba_model.u3.copy_to_host()))\n",
    "max = np.max(np.abs(model.u3.cpu().detach().numpy() - numba_model.u3.copy_to_host()))\n",
    "print(f'Mean Difference: {mean : .6f}')\n",
    "print(f'Max Difference: {max: .6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Difference:  0.000028\n",
      "Max Difference:  0.000770\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(np.abs(model.u4.cpu().detach().numpy() - numba_model.u4.copy_to_host()))\n",
    "max = np.max(np.abs(model.u4.cpu().detach().numpy() - numba_model.u4.copy_to_host()))\n",
    "print(f'Mean Difference: {mean : .6f}')\n",
    "print(f'Max Difference: {max: .6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Difference:  0.000504\n",
      "Max Difference:  0.011502\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(np.abs(out.cpu().detach().numpy()- out_numba))\n",
    "max = np.max(np.abs(out.cpu().detach().numpy()- out_numba))\n",
    "print(f'Mean Difference: {mean : .6f}')\n",
    "print(f'Max Difference: {max: .6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
